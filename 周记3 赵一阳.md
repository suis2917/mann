#### 特征归一化

用于数据预处理，使数据处在合理范围内，能使偏离值较为均匀的变化，使得梯度下降更便于找到最好值

![image-20240926213815164](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20240926213815164.png)

#### 逻辑回归

![image-20240927123549988](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20240927123549988.png)

#### 代价函数

逻辑回归的损失函数Loss  即期望若落空造成的损失

![image-20240926213916530](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20240926213916530.png)

**简化代价函数 **

![image-20240926214040272](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20240926214040272.png)

![image-20240926214058082](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20240926214058082.png)

这使得Loss也具有了碗的形状，使得逻辑回归也能使用梯度下降算法寻找极值



#### 逻辑回归模型

![img](https://i-blog.csdnimg.cn/blog_migrate/314f2f0da2c587794e6a53e4a2dcfdd2.png)



#### 简化代价函数与梯度下降

![img](https://i-blog.csdnimg.cn/blog_migrate/dd23a48ef34feb734b3d9c312c453335.png)

进而得到：

![img](https://i-blog.csdnimg.cn/blog_migrate/e2d4520a841c0c0e0cf74f97ee97d99d.png)

梯度下降：

![img](https://i-blog.csdnimg.cn/blog_migrate/4635508503b9ba2ea5b9ae3b2c9268e6.png)

**最后发现与正常的线性拟合公式相同**



#### 过拟合

当变量过多时，训练出来的假设能很好地拟合训练集，所以代价函数实际上可能非常接近于0，但得到的曲线为了千方百计的拟合数据集，导致它无法泛化到新的样本中，无法预测新样本数据（冗余项）



#### 正则化

正则化项：

![image-20241001120001255](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241001120001255.png)

如果有很多参数，我们**不清楚哪个参数是高阶项**，即**不知道惩罚哪个能获得更好拟合的结果**，因此**引入正则化项统一惩罚参数**以得到较为简单的函数

正则化后：

![image-20241001165228169](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241001165228169.png)

![image-20241001165329909](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241001165329909.png)

得到最终公式



### 神经网络

输入前一层的激活，给定当前层的参数，它会输出下一层激活值 自定义sequential函数
中间层为网络层、隐藏层，最后输出为激活层

####　前向传播

![image-20241001203807159](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241001203807159.png)

#### 激活函数

二元分类：sigmoid  默认用relu(不丢失有效值)  线性激活一般不使用

​    hidden layer 隐藏层
除非是二分类问题用sigmoid，一般默认用relu，不能用线性激活函数，否则不能拟合，就相当于一个线性回归了
因为relu计算很快，并且只有右边扁平flat，而sigmoid有两处flat，会影响gd的速度（因为激活函数是计算偏导的一部分）



#### Tensorflow 

 可以直接调用不同的损失函数来训练模型



#### 模型训练细节

**此处二元交叉熵作为损失函数**

![image-20241006150811799](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006150811799.png)

![image-20241006150826294](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006150826294.png)

使用二元交叉熵作为损失函数时，通常使用以下公式来计算单个样本的损失：
l o s s = − [ y l o g ( p ) + ( 1 − y ) l o g ( 1 − p ) ] loss = -[ylog(p) + (1-y)log(1-p)]loss=−[ylog(p)+(1−y)log(1−p)]
其中，y 是真实标签（0或1），p pp是预测值（0到1之间的概率值）。这个公式表示，当真实标签为1时，我们希望预测值p越接近1，此时损失越小，等于− l o g ( p ) -log(p)−log(p)；
当真实标签为0时，我们希望预测值p 越接近0，此时损失也越小，等于− l o g ( 1 − p ) -log(1-p)−log(1−p)。



#### softmax 回归算法

Softmax 回归算法的基本思想是将输入数据映射到多个类别之一，并为每个类别分配一个概率。算法的输入是一个 n 维特征向量 x ，输出是 K  个类别的概率分布，其中 K  是类别的数量。（用于分类多元）

![image-20241006151206182](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006151206182.png)

损失函数：

![image-20241006151304493](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006151304493.png)

#### softmax 改进实现，更加精确

理解不同的计算方式的精度是不同的，有的会在计算机存储过程中伴随值精度的损失(计组)
直接带入计算避免公式套用可以避免一些误差



#### 卷积神经网络

本质是特征提取

- **参数共享**：卷积操作中使用相同的滤波器减少了参数数量，提高了计算效率。
- **局部连接**：通过局部感受野提取图像局部特征，能够捕捉空间关系。



#### 可保留一部分数据集用于测试集，再一部分作为交叉验证集

先通过训练集拟合w,b. 然后通过交叉集选择出最优的模型也即参数d，最后用选择出来的模型在测试集中估计**泛化能力**，就是一个训练参数wb，一个挑选最合适的一个模型，test用来算误差（敲定最终模型，才能在test集中评估）

![image-20241006234421527](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006234421527.png)

#### 引入正则化

![image-20241006234504614](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006234504614.png)

1. 基准baseline performance 和Jtrain 之间的距离，可以判断是否有一个高偏差bias问题
2. Jtrain 和Jcv之间的距离可以判断是否有一个高方差variance

**偏差为是否拟合训练集 方差为是否拟合交叉验证集Jcv**



#### 迁移学习 transfer learning（监督预训练）

1. 微调方法一（**训练集很小**）
   **前4个参数仍然保留**，只需要更新输出层的参数，利用gd 或者adam优化算法来最小化代价函数
2. 微调方法二（**训练集很大**）
   重新训练所有参数，但**前四层的参数还是用上面的值作为初始值**！
