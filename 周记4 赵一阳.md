### 神经网络

输入前一层的激活，给定当前层的参数，它会输出下一层激活值 自定义sequential函数
中间层为网络层、隐藏层，最后输出为激活层

####　前向传播

![image-20241001203807159](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241001203807159.png)

#### 激活函数

二元分类：sigmoid  默认用relu(不丢失有效值)  线性激活一般不使用

​    hidden layer 隐藏层
除非是二分类问题用sigmoid，一般默认用relu，不能用线性激活函数，否则不能拟合，就相当于一个线性回归了
因为relu计算很快，并且只有右边扁平flat，而sigmoid有两处flat，会影响gd的速度（因为激活函数是计算偏导的一部分）



#### Tensorflow 

 可以直接调用不同的损失函数来训练模型



#### 模型训练细节

**此处二元交叉熵作为损失函数**

![image-20241006150811799](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006150811799.png)

![image-20241006150826294](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006150826294.png)

使用二元交叉熵作为损失函数时，通常使用以下公式来计算单个样本的损失：
l o s s = − [ y l o g ( p ) + ( 1 − y ) l o g ( 1 − p ) ] loss = -[ylog(p) + (1-y)log(1-p)]loss=−[ylog(p)+(1−y)log(1−p)]
其中，y 是真实标签（0或1），p pp是预测值（0到1之间的概率值）。这个公式表示，当真实标签为1时，我们希望预测值p越接近1，此时损失越小，等于− l o g ( p ) -log(p)−log(p)；
当真实标签为0时，我们希望预测值p 越接近0，此时损失也越小，等于− l o g ( 1 − p ) -log(1-p)−log(1−p)。



#### softmax 回归算法

Softmax 回归算法的基本思想是将输入数据映射到多个类别之一，并为每个类别分配一个概率。算法的输入是一个 n 维特征向量 x ，输出是 K  个类别的概率分布，其中 K  是类别的数量。（用于分类多元）

![image-20241006151206182](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006151206182.png)

损失函数：

![image-20241006151304493](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006151304493.png)

#### softmax 改进实现，更加精确

理解不同的计算方式的精度是不同的，有的会在计算机存储过程中伴随值精度的损失(计组)
直接带入计算避免公式套用可以避免一些误差



#### 卷积神经网络

本质是特征提取

- **参数共享**：卷积操作中使用相同的滤波器减少了参数数量，提高了计算效率。
- **局部连接**：通过局部感受野提取图像局部特征，能够捕捉空间关系。



#### 可保留一部分数据集用于测试集，再一部分作为交叉验证集

先通过训练集拟合w,b. 然后通过交叉集选择出最优的模型也即参数d，最后用选择出来的模型在测试集中估计**泛化能力**，就是一个训练参数wb，一个挑选最合适的一个模型，test用来算误差（敲定最终模型，才能在test集中评估）

![image-20241006234421527](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006234421527.png)

#### 引入正则化

![image-20241006234504614](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241006234504614.png)

1. 基准baseline performance 和Jtrain 之间的距离，可以判断是否有一个高偏差bias问题
2. Jtrain 和Jcv之间的距离可以判断是否有一个高方差variance

**偏差为是否拟合训练集 方差为是否拟合交叉验证集Jcv**



#### 迁移学习 transfer learning（监督预训练）

1. 微调方法一（**训练集很小**）
   **前4个参数仍然保留**，只需要更新输出层的参数，利用gd 或者adam优化算法来最小化代价函数

2. 微调方法二（**训练集很大**）
   重新训练所有参数，但**前四层的参数还是用上面的值作为初始值**！

   

#### **监督预训练supervised pretraining**

首先在大型数据集上进行训练，然后在较小的数据集上进行进行参数**微调**（fine tuning）利用已经初始化的或者从预训练模型获取的参数，运行gd，进一步进行微调权重，以适应手写数字识别任务



#### 倾斜数据集的误差指标

**某个类别的样本数只有另一个类别的1/10，这个数据集就是倾斜的**。这种情况也常常出现在诸如**诈骗检测、医学诊断、罕见事件预测**等领域中，因为这些问题中**正例的数量通常比负例少得多**。



#### 精确率（找的对）和召回率（找的全）的权衡

![image-20241011164321599](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241011164321599.png)

高精确率：判定为true的置信度高
高召回率：判定为false的置信度高

高准确率：分类正确的置信度高



#### 置信度

置信度是指一个统计推断中，对于一个参数或者假设所得到的推断结果的可信程度或者确定性程度。置信度通常使用置信区间来度量，表示参数或假设在一定置信水平下的可能取值范围。
置信度高表示推断所得的结果越可信，即在一定置信水平下，得到的置信区间越小，说明参数或假设的可能取值范围越小，结果越精确可靠。例如，如果某个样本的置信区间为[2, 6]，则在95%的置信水平下，该样本的真实值有95%的可能在区间[2, 6]内。如果置信度更高，比如99%的置信水平，则样本的真实值有99%的可能在更小的区间内，如[2.5, 5.5]。



#### 决策树

需要**最大限度地提高纯度**（或最小限度地减少不纯）（纯度：近似于正确率）

熵：![image-20241011164744962](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241011164744962.png)

可以发现，当正例和反例比例相等时，即p + = p − = 0.5 时，此时的熵取最大值1 ，表明此时样本集合最不确定，样本集合越不纯，熵值越大；而当样本集合中只包含一类样本（即纯样本集合）时，熵值为0 00，表明此时样本集合已经完全确定，很纯。

在决策树算法中，熵常常被用来选择最优划分点，即在样本的某个特征上根据熵的变化选取最优的划分点，使得划分后的样本子集中的熵最小。




#### 信息增益

在决策树算法中，我们希望通过选择最优特征来划分数据集，从而使划分后的数据集更加纯净，即熵减少。而这个熵减少的量就被称为信息增益

信息增益是一种用于衡量一个特征对分类任务的贡献程度的指标，通常用于决策树算法中。在决策树算法中，我们需要在每个节点上选择一个特征，使得选定特征后能够最大程度地提高数据的纯度（即分类的准确性）。信息增益的计算方法是：首先计算数据集的熵，然后计算选定特征后的条件熵，两者相减即为该特征的信息增益。信息增益越大，代表该特征对分类任务的贡献越大。



#### one-hot编码

它将每个类别值转换为一个长度为N的二进制向量，其中N是类别的总数。（01代替特征）



####　回归树

回归树是一种用于预测连续目标变量的决策树模型。它通过将数据集划分成若干个区域，来对每个区域内的目标值进行估计

从根节点开始，选择最佳特征及其切分点，将数据分割成不同的子集

在**划分时**，不去计算信息增益，不去衡量纯度，不如**减少数据的方差**，类似信息增益，**测量的是方差的减小，选择方差减少最大的那个特征分类标准**



#### 有放回抽样


构造多个随机的训练集



#### 随机森林

![image-20241012094046390](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241012094046390.png)

![image-20241012094105123](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241012094105123.png)

随机森林是一种集成学习算法，它由多个决策树组成，每个决策树之间相互独立

​    1随机选取n个样本作为训练集，对于每个样本，随机选取k个特征进行决策树的构建。通常情况下，n取样总数的70%到80%，k取特征总数的平方根。
​    2对于每个节点，随机选取k个特征进行划分，并在选取的特征中找到最优划分特征。这里的最优划分是通过计算信息增益或Gini系数等指标得出的。
​    3重复步骤1和步骤2，构建多棵决策树。
​    4预测时，将新数据输入到每个决策树中，得到每棵树的预测结果，然后根据投票法或平均值等方法，得到随机森林的最终预测结果。
​    随机森林算法具有良好的泛化性能和抗噪性能，通常被用于分类和回归任务。

#### XGBoost

在构建下一个决策树时，把更多的注意力放在做得不好的例子上，不是以相等的(1/m)概率从所有的例子中选取，而是**更有可能选取以前训练的树所错误分类的例子**

XGBoost的训练过程是一个逐步迭代的过程。它通过不断训练多棵决策树，并使用梯度下降来对每棵树的权重进行优化，以最小化损失函数。
具体来说，XGBoost在构建每棵树时，会先构建一个根节点，然后逐步将数据集分配到子节点中。每次分配数据集时，XGBoost都会根据一个指定的评价函数来选择最佳的特征进行分裂，并计算每个子节点的权重。通过这种方式，XGBoost可以在不断迭代中构建多棵树，并综合它们的预测结果来提高模型的准确率

XGBoost库是一个开源的机器学习库，专注于梯度提升决策树（GBDT）算法，广泛应用于分类和回归任务。



#### 何时使用决策树、神经网络

- 决策树

1. 决策树可以很好处理**表格数据（结构化数据）**
2. 训练很快
3. 小型的决策树具有可解释性

- 神经网络

1. 适用于所有类型的数据，表格和非结构化数据
2. 可能比较慢
3. 可以迁移学习、预训练

![image-20241012095658421](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241012095658421.png)



#### 聚类算法

第一步，把每个点分配给簇质量心，分配给离它最近的簇质心；第二步，把每个簇质心分配给它所有点的平均位置;直到簇质心没有变化

![image-20241014171114095](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241014171114095.png)

Ci，表示当前**训练样本Xi所在的簇**的索引1~k，是每个样本的分类标识
uk 是簇质心的位置
**当k=Ci时**，uci是样本Xi**被分配到的簇**，这个**簇的簇质心的位置**
m是所有样本个数

![image-20241014171145845](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241014171145845.png)



#### 聚类算法的损失函数

![image-20241014171211322](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20241014171211322.png)

算法每一步会**更新样本所属簇的类别即Ci的值**、
或者**移动簇质心的位置uk**，以降低代价函数



#### 初始化K-means

K-means 聚类算法第一步是为簇质心uj uk选择随机位置作为初始值

如果初始值选的不好，会陷入局部最小值，多次运行K-means，计算他们的J，选最小的那个


#### 如何选择聚类的簇的数量K

肘部法则 （一般不用）
运行具有各种K值的k-means，绘制代价函数-K的函数，找到肘关节位置，但比较主观



#### 异常检测

假设已知数据集由多个高斯分布组成，**每个高斯分布对应数据集中的一个簇**，那么可以通过最大化似然函数来估计这些高斯分布的参数，从而得到整个数据集的概率分布模型。具体来说，对于一个新的样本数据，可以计算它在该模型下的概率密度值，如果该值较小，则认为该样本是异常值。



#### 强化学习

在数学中，矩阵特征值和特征向量是很重要的概念，它们可以帮助我们理解和分析矩阵的性质和行为。其中，特征值是一个标量，表示矩阵沿着某个方向的缩放比例，而特征向量则是一个非零向量，表示在这个方向上的向量不变，只是被缩放了。

在机器学习中，我们通常会对数据进行特征提取和降维，其中一种常见的方法是使用主成分分析（Principal Component Analysis, PCA）。PCA通过对数据进行矩阵分解，将原始数据转化为一组线性无关的主成分，以实现数据降维的目的。




#### 一些Scikit-Learn库

```python
LogisticRegression().fit(X, y)  #在训练数据上拟合这个模型
y_pred = lr_model.predict(X) #预测
lr_model.score(X, y)) #计算在训练集上的准确率
```



到这里为止已经差不多了解了机器学习的很多基础算法和概念，也有了一点代码基础，再回头看李沐的视频障碍也少了很多，现在开始继续看李沐的教程，旨在了解一些经典的神经网络算法和大型神经网络，深化自己对深度学习的了解。
